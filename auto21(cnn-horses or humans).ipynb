{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KiHBCzg9M_wH",
        "outputId": "6cf7449b-b185-4928-fb96-f4c55ccb311f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Train.zip\n",
            "   creating: Train/\n",
            "   creating: Train/Test/\n",
            "   creating: Train/Test/Horses/\n",
            "  inflating: Train/Test/Horses/horse02-1.png  \n",
            "  inflating: Train/Test/Horses/horse02-2.png  \n",
            "  inflating: Train/Test/Horses/horse02-3.png  \n",
            "  inflating: Train/Test/Horses/horse02-4.png  \n",
            "  inflating: Train/Test/Horses/horse02-5.png  \n",
            "  inflating: Train/Test/Horses/horse02-6.png  \n",
            "  inflating: Train/Test/Horses/horse02-7.png  \n",
            "  inflating: Train/Test/Horses/horse02-8.png  \n",
            "   creating: Train/Test/Humans/\n",
            "  inflating: Train/Test/Humans/human01-11.png  \n",
            "  inflating: Train/Test/Humans/human01-12.png  \n",
            "  inflating: Train/Test/Humans/human01-13.png  \n",
            "  inflating: Train/Test/Humans/human01-14.png  \n",
            "  inflating: Train/Test/Humans/human01-15.png  \n",
            "  inflating: Train/Test/Humans/human01-16.png  \n",
            "  inflating: Train/Test/Humans/human01-17.png  \n",
            "  inflating: Train/Test/Humans/human01-18.png  \n",
            "   creating: Train/Train/\n",
            "   creating: Train/Train/Horses/\n",
            "  inflating: Train/Train/Horses/horse01-1.png  \n",
            "  inflating: Train/Train/Horses/horse01-2.png  \n",
            "  inflating: Train/Train/Horses/horse01-3.png  \n",
            "  inflating: Train/Train/Horses/horse01-4.png  \n",
            "  inflating: Train/Train/Horses/horse01-5.png  \n",
            "  inflating: Train/Train/Horses/horse01-6.png  \n",
            "  inflating: Train/Train/Horses/horse01-7.png  \n",
            "  inflating: Train/Train/Horses/horse01-8.png  \n",
            "   creating: Train/Train/Humans/\n",
            "  inflating: Train/Train/Humans/human01-01.png  \n",
            "  inflating: Train/Train/Humans/human01-02.png  \n",
            "  inflating: Train/Train/Humans/human01-03.png  \n",
            "  inflating: Train/Train/Humans/human01-04.png  \n",
            "  inflating: Train/Train/Humans/human01-05.png  \n",
            "  inflating: Train/Train/Humans/human01-06.png  \n",
            "  inflating: Train/Train/Humans/human01-07.png  \n",
            "  inflating: Train/Train/Humans/human01-08.png  \n"
          ]
        }
      ],
      "source": [
        "!unzip Train.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlQ2pYM1Oi5t",
        "outputId": "69440699-fb15-46b9-b85b-7b6d13db5b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  Train  Train.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "EpLi5qjtOi9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/Train/Train'\n",
        "test_dir = '/content/Train/Test'"
      ],
      "metadata": {
        "id": "fqEM4difOjA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=20,zoom_range=0.15,\n",
        "                                   width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,\n",
        "                                   horizontal_flip=True,fill_mode='nearest')"
      ],
      "metadata": {
        "id": "4JfpmP4AOjD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "BMsxVpWEPFPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YhKEDU8PFfI",
        "outputId": "b3c49b08-635d-4a17-fdd5-6d10e9c6fda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxTzp1IVPFlZ",
        "outputId": "617c394e-4e9c-4f20-99f6-d3bcb716b551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYcnVxlwPFo3",
        "outputId": "4df856ad-cbc0-47ca-ea72-b4907fa714e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fKeNvehEPm2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, epochs=20, validation_data=test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUFVltYsPm56",
        "outputId": "5a9f8dfa-5be4-4e00-c52b-0cccbd5b3602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.5625 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6925\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6250 - loss: 0.6791 - val_accuracy: 0.4375 - val_loss: 0.6996\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step - accuracy: 0.5625 - loss: 0.6909 - val_accuracy: 0.5000 - val_loss: 0.7060\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633ms/step - accuracy: 0.6875 - loss: 0.6301 - val_accuracy: 0.5000 - val_loss: 0.7065\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.6250 - loss: 0.6777 - val_accuracy: 0.2500 - val_loss: 0.6970\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.5000 - loss: 0.6959 - val_accuracy: 0.5000 - val_loss: 0.6971\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - accuracy: 0.6875 - loss: 0.5575 - val_accuracy: 0.5000 - val_loss: 0.6962\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.5625 - loss: 0.6433 - val_accuracy: 0.4375 - val_loss: 0.6965\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5625 - loss: 0.6635 - val_accuracy: 0.5000 - val_loss: 0.7016\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.6875 - loss: 0.6272 - val_accuracy: 0.5000 - val_loss: 0.7134\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.6250 - loss: 0.6849 - val_accuracy: 0.5000 - val_loss: 0.7175\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.7500 - loss: 0.5407 - val_accuracy: 0.5000 - val_loss: 0.7050\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.4375 - loss: 0.7787 - val_accuracy: 0.5000 - val_loss: 0.6955\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 0.6250 - loss: 0.6925 - val_accuracy: 0.5000 - val_loss: 0.6957\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.8125 - loss: 0.5236 - val_accuracy: 0.5000 - val_loss: 0.6960\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642ms/step - accuracy: 0.8125 - loss: 0.5618 - val_accuracy: 0.5625 - val_loss: 0.6924\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.6250 - loss: 0.6120 - val_accuracy: 0.4375 - val_loss: 0.6909\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587ms/step - accuracy: 0.7500 - loss: 0.5709 - val_accuracy: 0.5000 - val_loss: 0.7049\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.6875 - loss: 0.5782 - val_accuracy: 0.5000 - val_loss: 0.7228\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step - accuracy: 0.5625 - loss: 0.6124 - val_accuracy: 0.5000 - val_loss: 0.7309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel.keras')"
      ],
      "metadata": {
        "id": "dVfm3oNRPm9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "def predict_image(img_path,model):\n",
        "  img = image.load_img(img_path,target_size=(150,150))\n",
        "  img_tensor =image.img_to_array(img)\n",
        "  img_tensor = np.expand_dims(img_tensor,axis=0)\n",
        "  img_tensor /=255.\n",
        "  prediction = model.predict(img_tensor)\n",
        "  if prediction[0] > 0.5:\n",
        "    print('The image is a Human !')\n",
        "  else:\n",
        "    print('The image is a Horse !')"
      ],
      "metadata": {
        "id": "tjyKNuhIPnAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_image('/content/horse01-7.png',model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrpHGAf1QTnQ",
        "outputId": "368547dd-ddea-456c-b04d-91e1e6ed7efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "The image is a Horse !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_image('/content/human01-02.png',model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6XlOahrQjDF",
        "outputId": "9710d1eb-4ac5-4b86-bab9-c37edc8ddd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "The image is a Human !\n"
          ]
        }
      ]
    }
  ]
}